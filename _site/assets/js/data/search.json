[
  
  {
    "title": "BYO Waypoint in Istio Ambient Mode - Expanding Your Service Mesh Options",
    "url": "/posts/byo-waypoint/",
    "categories": "Kubernetes, Networking",
    "tags": "istio, ambient, waypoint, envoy, service mesh, kubernetes",
    "date": "2025-04-02 00:00:00 +0530",
    





    
    "snippet": "Introducing BYO Waypoint in Istio Ambient ModeIstio Ambient’s architecture continues to evolve with powerful new capabilities that enhance flexibility and integration options. One such capability i...",
    "content": "Introducing BYO Waypoint in Istio Ambient ModeIstio Ambient’s architecture continues to evolve with powerful new capabilities that enhance flexibility and integration options. One such capability is “Bring Your Own” (BYO) waypoint functionality, enabled through Ambient’s “sandwich mode” deployment model. This approach allows any Kubernetes Gateway API conformant proxy to be deployed as a waypoint, opening up new possibilities for extending your service mesh.Understanding the Waypoint in Ambient ArchitectureBefore diving into BYO waypoint capabilities, let’s revisit very briefly how waypoints function in the standard Ambient deployment model.Standard Istio Ambient architecture with ztunnels and waypointIn this standard configuration:  Client Pod’s(app container’s) traffic gets redirected to the local ztunnel proxy running in its own network namespace.  The ztunnel sends traffic via HBONE (HTTP-Based Overlay Network Environment) to the waypoint for L7 processing  After processing, the waypoint sends traffic via HBONE to the destination ztunnel  Finally, the destination ztunnel(running inside server pod’s network namespace) delivers traffic to the app container.The ztunnel DaemonSet on each node provisions proxies within pod network namespaces but isn’t directly in the traffic path. All HBONE connections are secured with mTLS.Rather than revisiting the fundamentals of Ambient mode that are well-documented in the official Istio documentation and existing community resources, this post focuses specifically on the “sandwich mode” architecture and its implications for extending service mesh capabilities.The “Sandwich Mode”The “sandwich mode” deployment represents a significant architectural innovation. Instead of using the standard istio-waypoint class for waypoint deployment, any Kubernetes Gateway API conformant proxy, such as EnvoyGateway can be used as a waypoint:Sandwich Mode: Using Gateway Class for WaypointKey aspects of this architecture:  The waypoint is deployed using ingress gateway class istio unlike istio-managed waypoint where istio-waypoint is used  It’s labeled with istio.io/dataplane-mode: ambient to enable traffic capture, unlike istio-managed waypoint where waypoint traffic is not intercepted by ztunnel  Server pods specify this non-istio gateway/waypoint as their waypoint using the usual use-waypoint label  Traffic to this waypoint is intercepted by ztunnel, which terminates the HBONE connection and uses plaintext within the waypoint’s network namespace. Please note because of ztunnel interception, we dont need to configure any mtls or HBONE at custom waypoint proxy.  The waypoint proxy can be configured using HTTPRoute to apply L7 processing before routing to the server pod  The outgoing waypoint traffic also gets intercepted by ztunnel(within the same network namespace) and forwarded via HBONE to the server pod, where another ztunnel terminates the connection.Let’s look at the zoomed-in view of how this works within the waypoint node:Detailed view of Gateway-class Waypoint in Sandwich ModeThis diagram shows how traffic flows through the waypoint:  HBONE traffic arrives at the waypoint’s ztunnel proxy (port 15008)  The ztunnel terminates HBONE and forwards to the Gateway proxy via plaintext (within the same network namespace)  After L7 processing, which could be configured using any non-istio control plane,the Gateway proxy returns traffic to the ztunnel  The ztunnel forwards traffic via HBONE to the destinationBringing Your Own Waypoint: The EnvoyGateway ExampleThe most powerful aspect of this architecture is the ability to use alternative Gateway API conformant proxies as waypoints. This means you can leverage specialized proxies with advanced features that may not be available in Istio’s standard waypoint implementation.For example, you can use EnvoyGateway as your waypoint control plane while still benefiting from Istio’s service mesh foundation:BYO Waypoint with EnvoyGateway Control PlaneThis architecture provides several significant advantages:  Istio still manages service discovery, certificates and ztunnel configuration  The alternative control plane (like EnvoyGateway) manages the waypoint proxy configurations  Both systems coexist within the same mesh architectureAdvantages of Using Alternative Gateway API Conformant Proxies  Leverage existing investments: Use proxies your team already knows and has deployed  Best-of-breed approach: Select specialized proxies with capabilities tailored to your needs  Advanced features: Access capabilities like sophisticated rate limiting, custom filters, and authentication methods that may not be available in Istio  Operational efficiency: Maintain familiar debugging workflows and monitoring tools  Ecosystem integration: Better align with your existing API gateway and ingress solutionsUse Cases Where This Flexibility Matters  Advanced traffic management: When you need sophisticated features such as rate limiting or AI gateway capabilities  Security requirements: For specialized authentication protocols or custom security filters  Multi-team environments: Different teams can use their preferred proxies while maintaining mesh cohesion  Gradual migration: Adopt Ambient mode while preserving existing gateway investments  Feature requirements: Access specialized proxy capabilities not yet available in IstioImportant: The integration between EnvoyGateway and Istio Ambient’s waypoint functionality is currently under active development. Keep an eye on the EnvoyGateway project for updates as this feature progresses.Implementation DetailsTo deploy a waypoint in “sandwich mode”, you need to:  Deploy a gateway using a standard gateway class (not istio-waypoint)  Label it with istio.io/dataplane-mode: ambient  Configure your workloads to use this gateway as their waypoint with appropriate labels  Configure HTTPRoute or other gateway resources to define L7 routing and policies# Example Gateway in \"sandwich mode\"apiVersion: gateway.networking.k8s.io/v1beta1kind: Gatewaymetadata:  name: byo-waypoint  namespace: my-ns  labels:    istio.io/dataplane-mode: ambientspec:  gatewayClassName: istio # Using standard gateway class  listeners:  - allowedRoutes:      namespaces:        from: Same    hostname: my-svc.my-ns.svc.cluster.local    name: captured-fqdn    port: 80    protocol: HTTP  - allowedRoutes:      namespaces:        from: Same    name: fake-hbone-port    port: 15008    protocol: TCP---# Example HTTPRoute for L7 routingapiVersion: gateway.networking.k8s.io/v1beta1kind: HTTPRoutemetadata:  name: example-route  namespace: my-nsspec:  hostnames:  - my-svc.my-ns.svc.cluster.local  parentRefs:  - group: gateway.networking.k8s.io    kind: Gateway    name: byo-waypoint  rules:  - matches:    - path:        type: PathPrefix        value: /    backendRefs:    - group: \"\"        kind: Service        name: my-svc        port: 80If you are a hands-on kind of person like me, then trying out istio integration test TestSimpleHTTPSandwich can be a good starting point.ConclusionBYO Waypoint functionality in Istio Ambient mode represents a significant advancement in service mesh architecture. It embraces the Kubernetes Gateway API ecosystem while maintaining the security and connectivity benefits of the Ambient mesh.This approach gives operators unprecedented flexibility to:  Leverage specialized gateway implementations  Access advanced traffic management features  Integrate with existing infrastructure  Gradually adopt Ambient modeAs service mesh technology continues to evolve, this kind of architectural flexibility will be crucial for accommodating diverse workloads and infrastructure requirements while maintaining a consistent security and observability foundation.I welcome your thoughts and questions on this complex topic in the comments section below, or reach out to me directly to continue the conversation!"
  },
  
  {
    "title": "Enhancing Envoy's Local Rate Limiting with Dynamic Token Buckets",
    "url": "/posts/dynamic-token-buckets/",
    "categories": "Envoy, Rate Limiting",
    "tags": "envoy-proxy, rate-limiting, token-bucket, http-filter",
    "date": "2025-03-10 17:30:00 +0530",
    





    
    "snippet": "IntroductionEnvoy Proxy has long offered powerful rate limiting capabilities to protect services from traffic spikes and ensure fair resource allocation. However, until recently, there was a signif...",
    "content": "IntroductionEnvoy Proxy has long offered powerful rate limiting capabilities to protect services from traffic spikes and ensure fair resource allocation. However, until recently, there was a significant limitation in how users could configure local rate limiting descriptors.While Envoy allowed flexible extraction of values from HTTP requests at runtime based on user configuration, the descriptors that defined rate limiting rules required static, pre-defined values. This meant that to implement per-entity rate limiting (such as per-user, per-client, per-IP, etc.), you would either need to:  Configure an external rate limiting service (implementing global rate limiting, which adds latency and complexity), or  Use local rate limiting and statically define every possible identifier value in your configuration (impractical for dynamic environments)This gap between flexible value(http request properties) extraction and rigid descriptor definition was especially problematic for platform teams trying to implement fair usage policies across different request attributes without adding external dependencies.I recently contributed an enhancement to Envoy Proxy that addresses this limitation by allowing wildcard values in rate limit descriptor entries. This change enables dynamic creation of token buckets based on unique values extracted from requests, such as user identifiers, client IDs, API keys, or any other request property, without requiring an external rate limit service. It was a long-requested capability from both major Envoy users and the wider community (see GitHub issues #23351 and #19895).In this blog post, I’ll explain how this feature works, show configuration examples, and demonstrate practical applications for request-scoped rate limiting.Local vs Global Rate Limiting in EnvoyBefore diving into the enhancement details, it’s important to understand the two approaches to rate limiting in Envoy:flowchart LR    %% Define clients    client1([Client])    client2([Client])    %% LOCAL RATE LIMITING    subgraph \"Local Rate Limiting\"        direction TB        client1 --&gt; envoy1        subgraph \"Envoy Instance 1\"            envoy1[Envoy Proxy 1]            tb1[Token Bucket&lt;br/&gt;for user1]            envoy1 --&gt; tb1        end        client2 --&gt; envoy2        subgraph \"Envoy Instance 2\"            envoy2[Envoy Proxy 2]            tb2[Token Bucket&lt;br/&gt;for user1]            envoy2 --&gt; tb2        end        tb1 --&gt; backend1[(Service)]        tb2 --&gt; backend2[(Service)]        note1[No coordination between instances&lt;br/&gt;Same user gets separate limits]    end    %% GLOBAL RATE LIMITING    subgraph \"Global Rate Limiting\"        direction TB        client1 --&gt; genvoy1        client2 --&gt; genvoy2        subgraph \"Envoy Instances\"            genvoy1[Envoy Proxy 1]            genvoy2[Envoy Proxy 2]        end        genvoy1 --RPC--&gt; rls        genvoy2 --RPC--&gt; rls        subgraph \"Rate Limit Service\"            rls[Rate Limit Service]            gtb[Token Bucket&lt;br/&gt;for user1]            rls --&gt; gtb        end        rls -.Allow/Deny.-&gt; genvoy1 &amp; genvoy2        genvoy1 --&gt; gbackend1[(Service)]        genvoy2 --&gt; gbackend2[(Service)]        note2[Consistent limits across instances&lt;br/&gt;Requires network calls]    end    %% Styling    classDef proxy fill:#bbdefb,stroke:#1565c0,stroke-width:2px;    classDef bucket fill:#ffffff,stroke:#1565c0,stroke-width:1px;    classDef backend fill:#e8eaf6,stroke:#3f51b5,stroke-width:1px;    classDef note fill:#fffde7,stroke:#fbc02d,stroke-width:1px;    classDef gproxy fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px;    classDef gservice fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px;    classDef gbucket fill:#ffffff,stroke:#2e7d32,stroke-width:1px;    class envoy1,envoy2 proxy;    class genvoy1,genvoy2 gproxy;    class tb1,tb2 bucket;    class gtb gbucket;    class rls gservice;    class backend1,backend2,gbackend1,gbackend2 backend;    class note1,note2 note;Local Rate Limiting  Rate limits are applied independently on each Envoy instance  No network calls or external dependencies  Lower latency and higher reliability  Ideal for per-instance protection and simple use cases  Prior to this enhancement, could only rate limit against statically defined valuesGlobal Rate Limiting  Relies on an external rate limiting service  Consistent rate limiting across all Envoy instances  Requires network calls, adding latency and potential points of failure  Necessary for cluster-wide coordinated rate limiting  More flexible but with higher operational complexityThe dynamic descriptor enhancement I contributed focuses on improving local rate limiting, making it almost as flexible as global rate limiting but without the operational complexity.Background on Envoy Rate LimitingBefore diving into the enhancement, let’s briefly understand how Envoy’s local rate limiting traditionally worked.Envoy’s rate limiting is configured through descriptors which contain entries with key-value pairs. These descriptors define the conditions under which rate limiting is applied. When a request comes in, Envoy extracts values from the request (such as headers, path, or method) and checks if they match the configured descriptor entries.For example, if you wanted to rate limit requests from a specific client:descriptors:- entries:  - key: \"client-id\"    value: \"premium-client\"  token_bucket:    max_tokens: 100    tokens_per_fill: 100    fill_interval: 60sThis configuration would only apply rate limiting to requests where the client-id header equals premium-client. If you wanted to rate limit multiple clients differently, you would need separate descriptor entries for each client ID:descriptors:- entries:  - key: \"client-id\"    value: \"premium-client\"  token_bucket:    max_tokens: 100    tokens_per_fill: 100    fill_interval: 60s- entries:  - key: \"client-id\"    value: \"standard-client\"  token_bucket:    max_tokens: 50    tokens_per_fill: 50    fill_interval: 60sThis approach becomes unwieldy when you have many clients or when client identifiers are not known in advance.For more detailed information on Envoy’s rate limiting API, you can refer to the official documentation.Traditional Rate Limiting Flowflowchart TD    subgraph \"Configuration Time\"        Config[\"User Configuration:&lt;br/&gt;Descriptor Entry&lt;br/&gt;Key: 'client-id'&lt;br/&gt;Value: 'premium-client'\"]    end    subgraph \"Request Processing Time\"        R[HTTP Request] --&gt; Extract[\"Extract Value from Request&lt;br/&gt;Header 'client-id' = 'premium-client'\"]        Extract --&gt; Match{\"Does Extracted Value&lt;br/&gt;Match Configured Value?\"}        Match --&gt;|Yes| Apply[\"Apply Rate Limit:&lt;br/&gt;Use Token Bucket for 'premium-client'\"]        Match --&gt;|No| Skip[\"Skip Rate Limiting&lt;br/&gt;for this Request\"]    end    Config -.-&gt;|Configures| Match    subgraph \"Limitation\"        Problem[\"Problem: Need Separate Configuration&lt;br/&gt;for Each Client ID\"]    end    class Config blue;    class Problem red;    class Match yellow;    classDef blue fill:#e3f2fd,stroke:#1565c0,stroke-width:2px;    classDef red fill:#ffebee,stroke:#c62828,stroke-width:2px;    classDef yellow fill:#fff8e1,stroke:#ffa000,stroke-width:2px;In the traditional approach, each configured descriptor entry requires an exact match for the value. This means you need to know all possible values ahead of time and configure them explicitly.The Enhancement: Dynamic Descriptor EntriesThe enhancement I contributed allows users to leave the value field blank in descriptor entries, which functions as a “wildcard”. This means Envoy will dynamically create and manage separate token buckets for each unique value it encounters at runtime.Here’s how the configuration looks with this enhancement:descriptors:- entries:  - key: \"client-id\"    value: \"\" # Wildcard - matches any value  token_bucket:    max_tokens: 100    tokens_per_fill: 100    fill_interval: 60sWith this single configuration, if Envoy receives requests with client-id headers containing “client-A”, “client-B”, and “client-C”, it will automatically create three separate token buckets and apply rate limiting independently to each client.Enhanced Dynamic Rate Limiting Flowflowchart TD    subgraph \"Configuration Time\"        Config[\"Enhanced Configuration:&lt;br/&gt;Descriptor Entry&lt;br/&gt;Key: 'client-id'&lt;br/&gt;Value: '' (Wildcard)\"]    end    subgraph \"Request Processing Time\"        R1[Request 1&lt;br/&gt;Header 'client-id' = 'client-A'] --&gt; Extract1[\"Extract Value&lt;br/&gt;'client-A'\"]        R2[Request 2&lt;br/&gt;Header 'client-id' = 'client-B'] --&gt; Extract2[\"Extract Value&lt;br/&gt;'client-B'\"]        Extract1 --&gt; Wildcard1{\"Is Configured Value&lt;br/&gt;a Wildcard?\"}        Extract2 --&gt; Wildcard2{\"Is Configured Value&lt;br/&gt;a Wildcard?\"}        Wildcard1 --&gt;|Yes| DynamicBucket1[\"Dynamically Create/Use&lt;br/&gt;Token Bucket for 'client-A'\"]        Wildcard2 --&gt;|Yes| DynamicBucket2[\"Dynamically Create/Use&lt;br/&gt;Token Bucket for 'client-B'\"]        DynamicBucket1 --&gt; Apply1[\"Apply Rate Limit&lt;br/&gt;for 'client-A'\"]        DynamicBucket2 --&gt; Apply2[\"Apply Rate Limit&lt;br/&gt;for 'client-B'\"]    end    Config -.-&gt;|Configures| Wildcard1 &amp; Wildcard2    subgraph \"Benefit\"        Solution[\"Benefit: Single Configuration&lt;br/&gt;Handles All Client IDs\"]    end    class Config blue;    class Solution green;    class DynamicBucket1,DynamicBucket2 purple;    classDef blue fill:#e3f2fd,stroke:#1565c0,stroke-width:2px;    classDef green fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px;    classDef purple fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px;The enhanced approach creates token buckets dynamically based on the unique values, for the keys specified by user in configuration,  extracted from incoming requests. A single configuration can now handle any number of unique values.This approach offers several advantages:  Simplified configuration - no need to list every possible value  Future-proof - automatically handles new client IDs without reconfiguration  No external dependencies - all rate limiting happens locally in Envoy  Fine-grained control - each unique value gets its own rate limitImplementation DetailsThe key challenge in implementing this enhancement was ensuring that dynamically created descriptors wouldn’t consume unlimited resources. To address this, I implemented an LRU (Least Recently Used) cache mechanism to manage the dynamic descriptors.LRU Cache for Dynamic DescriptorsWhen Envoy encounters a request with a new unique value for a wildcard descriptor entry:  It creates a new token bucket for this value  It adds this descriptor to the LRU cache  If the cache reaches its capacity limit, the least recently used descriptor is evictedThe default cache size is set to 20 entries, which is a conservative value suitable for testing. However, in production environments with high request rates and many unique values, this limit might need to be increased to prevent premature eviction.When a descriptor is evicted and then encountered again, Envoy will create a new token bucket, effectively resetting the rate limit for that value. This could lead to unexpected behavior where some clients get more requests than intended if the cache size is too small relative to the number of unique values.Configuration ExampleHere’s how you can configure the cache size, max_dynamic_descriptors:route_config:  name: local_route  virtual_hosts:  - name: local_service    domains: [\"*\"]    routes:    - match: {prefix: \"/foo\"}      route:        cluster: service_protected_by_rate_limit        rate_limits:        - actions:  # any actions in here          - request_headers:              header_name: x-envoy-downstream-service-cluster              descriptor_key: client_cluster          - request_headers:              header_name: \":path\"              descriptor_key: path      typed_per_filter_config:        envoy.filters.http.local_ratelimit:          \"@type\": type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit          max_dynamic_descriptors: 10000          stat_prefix: test          token_bucket:            max_tokens: 1000            tokens_per_fill: 1000            fill_interval: 60s          filter_enabled:            runtime_key: test_enabled            default_value:              numerator: 100              denominator: HUNDRED          descriptors:          - entries:            - key: client_cluster              value: foo            - key: path              value: /foo/bar            token_bucket:              max_tokens: 10              tokens_per_fill: 10              fill_interval: 60s          - entries:            - key: client_cluster              value: foo            - key: path              value: /foo/bar2            token_bucket:              max_tokens: 100              tokens_per_fill: 100              fill_interval: 60sReal-World AdoptionIt’s worth noting that Envoy Gateway, a control plane that uses Envoy Proxy as its data plane, has already adopted this feature with a default cache size of 10,000 entries. This larger default acknowledges the practical needs of production environments with many unique clients.Usage ExamplesLet’s explore some real-world scenarios where this enhancement shines:Per-User API Rate LimitingLimit each user to 100 requests per minute based on a user ID header:max_dynamic_descriptors: 10000descriptors:- entries:  - key: \"user-id\"    value: \"\" # Wildcard - matches any user  token_bucket:    max_tokens: 100    tokens_per_fill: 100    fill_interval: 60sTiered Rate Limiting with Path DifferentiationApply different rate limits based on the API path for each client:max_dynamic_descriptors: 10000descriptors:- entries:  - key: \"client-id\"    value: \"\" # Wildcard - matches any client  - key: \"path\"    value: \"/api/v1/critical\" # Specific path  token_bucket:    max_tokens: 10    tokens_per_fill: 10    fill_interval: 60s- entries:  - key: \"client-id\"    value: \"\" # Wildcard - matches any client  - key: \"path\"    value: \"/api/v1/standard\" # Different path  token_bucket:    max_tokens: 100    tokens_per_fill: 100    fill_interval: 60sThis configuration creates separate rate limits for each client accessing each path, without requiring you to know the client IDs in advance.IP-Based Rate LimitingProtect your services from potential abuse by limiting requests per IP address:max_dynamic_descriptors: 20000descriptors:- entries:  - key: \"remote_address\"    value: \"\" # Wildcard - matches any IP  token_bucket:    max_tokens: 50    tokens_per_fill: 50    fill_interval: 60sPerformance ConsiderationsThe implementation is designed to be efficient, with the LRU cache mechanism providing a good balance between flexibility and resource usage. However, there are a few considerations to keep in mind:  Memory Usage: Each dynamic descriptor consumes memory. Set the max_entries appropriately for your environment.  Eviction Behavior: If a descriptor is evicted and then re-encountered, its rate limit counter effectively resets. Size your cache appropriately to minimize unwanted evictions.ConclusionThe addition of dynamic descriptor entries to Envoy’s local rate limiting capabilities represents a significant enhancement to Envoy’s traffic management toolkit. By allowing wildcard values in descriptor entries, we’ve simplified configuration while making it possible to apply fine-grained rate limits based on any request attribute without prior knowledge of all possible values.This feature has been a long-requested capability in the Envoy community, and I’m pleased to see it already being adopted in projects like Envoy Gateway. By enabling dynamic per-entity rate limiting without external dependencies, we’ve made it easier for service owners to implement fair usage policies and protect their services from excessive traffic.This enhancement is particularly valuable for the numerous control plane projects that use Envoy as their data plane, including Istio, Contour, Emissary, and Envoy Gateway. These projects can now provide their users with more flexible rate limiting options without requiring external rate limiting services.Next StepsIf you’re interested in using this feature:  Ensure you’re running a version of Envoy that includes this enhancement (post-February 2024)  Update your local rate limit filter configurations to use empty values for descriptor entries where you want dynamic behavior  Configure appropriate max_dynamic_descriptors values based on your expected number of unique values  Monitor memory usage and adjust as neededIf you’re a maintainer or user of a control plane that uses Envoy as its data path, I encourage you to explore this feature and consider how it might simplify your rate limiting strategies.Feedback and contributions are always welcome!!!!."
  },
  
  {
    "title": "Istio CNI Implementation in OpenShift",
    "url": "/posts/istio-cni-implementation-in-openshift/",
    "categories": "Kubernetes, Networking",
    "tags": "istio, openshift, cni, networking, service-mesh",
    "date": "2025-03-09 17:30:00 +0530",
    





    
    "snippet": "OverviewIstio CNI is a component of the Istio service mesh that modifies pod networking to transparently redirect traffic through the service mesh. In OpenShift environments, Istio CNI has specific...",
    "content": "OverviewIstio CNI is a component of the Istio service mesh that modifies pod networking to transparently redirect traffic through the service mesh. In OpenShift environments, Istio CNI has specific configuration and operational aspects that differ from standard Kubernetes deployments.Components and DeploymentIstio CNI is deployed as a daemonset named istio-cni-node. This daemonset runs on all cluster nodes and performs two primary functions:  Deploy CNI Plugin Binary: The container image contains the CNI plugin binary, which it copies to the node’s CNI plugin directory.  Generate CNI Configuration: The daemonset creates the appropriate CNI configuration file on each node.Path Differences: Standard K8s vs OpenShift            Resource      Standard K8s Path      OpenShift Path                  CNI Plugin Binary      /opt/cni/bin      /var/lib/cni/bin              CNI Config Files      /etc/cni/net.d/      /etc/cni/multus/net.d      Configuration TypeThe CNI configuration can be generated in two formats:  .conf file (when chained-cni=false)  .conflist file (when chained-cni=true)Important: In OpenShift, chained-cni is set to false in the helm values when rendering the CNI daemonset. This causes the generation of a .conf file rather than a .conflist file.DiagramPlugin Invocation ProcessIn Kubernetes, CNI plugins are invoked by container runtimes when pods are created or deleted. In OpenShift:  The container runtime is CRI-O  CRI-O scans pod annotations for the k8s.v1.cni.cncf.io/networks key  When a pod has the annotation k8s.v1.cni.cncf.io/networks: istio-cni (or including istio-cni in a comma-separated list), CRI-O recognizes it needs to invoke the Istio CNI plugin  CRI-O searches for a NetworkAttachmentDefinition resource:          If the annotation contains no namespace prefix (e.g., just istio-cni), it searches in the pod’s namespace      If the annotation includes a namespace (e.g., default/istio-cni), it searches in that specific namespace        Once found, CRI-O invokes the plugin binary from /var/lib/cni/bin/istio-cniExample Pod Annotationmetadata:  annotations:    k8s.v1.cni.cncf.io/networks: istio-cniAlternatively, to use a NetworkAttachmentDefinition from a different namespace:metadata:  annotations:    k8s.v1.cni.cncf.io/networks: default/istio-cniNetworkAttachmentDefinition ResourceThe NetworkAttachmentDefinition acts as a reference point for CRI-O to find and invoke the appropriate CNI plugin. This is created as part of the Istio CNI installation process and referenced by pod annotations.Customer Issue and Solution: Init Container Traffic and Istio RedirectionBackgroundWe encountered a customer issue in an OpenShift environment where init containers needed to bypass Istio traffic redirection since the istio-proxy container isn’t ready to handle redirected traffic during init phase.The ProblemPrior to Istio 1.20, this was handled by running init containers with UID 1337 (the same as istio-proxy), as Istio CNI included rules to skip redirection for traffic from this UID. Beginning with Istio 1.20, this approach stopped working because:  Istio changed how it selects UIDs in OpenShift environments  Instead of using a fixed UID 1337, Istio now selects the last UID from the namespace annotation openshift.io/sa.scc.uid-range  This broke existing deployments where vault init containers were hardcoded to use UID 1337. The vault init container was hardcoded based on earlier Istio CNI rules to skip redirection for 1337 UID.  The customer needed a centralized solution that wouldn’t require application teams to modify their deployments. Removing hard-coded UID from vault init containers will required modifying all their existing deployments.Solution ApproachesWe considered several approaches:  Using excludeOutboundIPRanges:          This Istio feature skips redirection for traffic to specific IP subnets      Limitation: Works only for TCP traffic, not for UDP-based DNS queries      DNS queries would still be redirected to port 15053, failing because istio-proxy wasn’t ready        Setting hostAliases:          Add static DNS resolution in pod specs and use excludeOutboundIPRanges      Rejected because it would require modifications to all application deployments**        Using k8s native sidecars for istio-proxy          If we look at real cause of the problem, it is because istio-proxy is not ready to handle the redirected traffic from the init containers.      k8s native sidecars can solve this issue because then istio-proxy would be run as an ever running init container. You can read more here      We could not go this path because customer was not at the minimum required k8s(openshift) version for native sidecar support.        Custom CNI Plugin Solution (Selected Approach):          Develop a shell-based CNI plugin to add iptables rules skipping redirection for UID 1337      Deploy via ConfigMap and DaemonSet      Add NetworkAttachmentDefinition for the custom plugin      Modify sidecar-injection-template to include the custom plugin in network annotations      Implementation DetailsThe solution uses a custom CNI plugin with these components:  A ConfigMap containing:          CNI configuration file (99-exclude-dns.conf)      Shell script implementing the CNI plugin logic        A DaemonSet that:          Mounts the ConfigMap      Copies the plugin to /var/lib/cni/bin      Copies the configuration to /etc/cni/multus/net.d        The CNI plugin adds an iptables rule to allow traffic from UID 1337 to bypass redirection# Example CNI configuration{  \"kubernetes\": {    \"cni_bin_dir\": \"/var/lib/cni/bin\",    \"exclude_namespaces\": [      \"istio-system\",      \"kube-system\"    ]  },  \"name\": \"excludedns\",  \"type\": \"excludedns\"}The DaemonSet installs this configuration and the script on all nodes, providing a centralized solution that doesn’t require application teams to modify their deployments.Debugging Istio CNI in OpenShiftWhen troubleshooting Istio CNI issues in OpenShift, follow these steps:1. Verify Pod AnnotationsCheck if the pod has the proper network annotation:oc get pod &lt;pod-name&gt; -o jsonpath='{.metadata.annotations.k8s\\.v1\\.cni\\.cncf\\.io/networks}'2. Verify NetworkAttachmentDefinition CREnsure the NetworkAttachmentDefinition exists in the correct namespace:oc get network-attachment-definitions -n &lt;namespace&gt;oc describe network-attachment-definition &lt;name&gt; -n &lt;namespace&gt;3. Examine Node CNI ConfigurationSSH to the node where the problematic pod is running:oc debug node/ip-10-0-15-84.us-east-2.compute.internalOnce on the node, check CNI config files:# Access host filesystemchroot /host# Check CNI binaries locationls -la /var/lib/cni/bin/# Check CNI configuration directoryls -la /etc/cni/multus/net.d/# Examine CNI config contentcat /etc/cni/multus/net.d/&lt;config-file&gt;.conf4. Check CRI-O LogsLook for CNI-related errors in the CRI-O logs:journalctl -u crio | grep -i cni5. Examine Container Details with crictlList all containers:crictl psFind the specific container ID for your pod:crictl ps | grep &lt;pod-name&gt;6. Determine Container PIDGet the process ID for container inspection:crictl inspect &lt;container-id&gt; | grep pid# Example output: \"pid\": 12345,7. Inspect iptables Rules in Container NamespaceView the NAT table rules in the container’s network namespace:nsenter -t &lt;pid&gt; -n iptables -L -n -v -t natThis lets you verify if the expected Istio redirection rules are present and if any custom rules (like those from the custom CNI plugin) are working as expected.SummaryIn OpenShift environments, Istio CNI operates with specific path differences and configuration settings compared to standard Kubernetes. Understanding these details is critical both for normal operation and for implementing solutions to issues like the init container traffic redirection problem described above.The key elements are:  Modified filesystem paths for CNI components  Use of .conf files instead of .conflist files  Reliance on pod annotations and NetworkAttachmentDefinition resources for plugin invocation  Integration with CRI-O as the container runtime  Ability to extend with custom CNI plugins for specialized requirements"
  },
  
  {
    "title": "Istio MTLS Smartness Explained",
    "url": "/posts/istio-mtls/",
    "categories": "Kubernetes, Networking",
    "tags": "istio, security",
    "date": "2022-03-09 17:30:00 +0530",
    





    
    "snippet": "https://vikaschoudhary16.wordpress.com/2022/06/20/undeistio-permissive-authz-magic-2/",
    "content": "https://vikaschoudhary16.wordpress.com/2022/06/20/undeistio-permissive-authz-magic-2/"
  },
  
  {
    "title": "Understanding Istio’s Secure Naming",
    "url": "/posts/istio-secure-naming/",
    "categories": "Kubernetes, Networking",
    "tags": "istio, security",
    "date": "2022-03-09 17:30:00 +0530",
    





    
    "snippet": "https://vikaschoudhary16.wordpress.com/2022/05/26/secure-naming-in-istio/",
    "content": "https://vikaschoudhary16.wordpress.com/2022/05/26/secure-naming-in-istio/"
  }
  
]

